{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n# 1) Define your token, repo URL and local directory name\nGIT_TOKEN = \"\"\nREPO_URL  = f\"https://{GIT_TOKEN}@github.com/semilleroCV/BreastCATT.git\"\nREPO_DIR  = \"BreastCATT\"\n\n# 2) Clone or pull depending on whether the folder exists\nif not os.path.isdir(REPO_DIR):\n    print(f\"Cloning repository into ./{REPO_DIR}â€¦\")\n    os.system(f\"git clone {REPO_URL}\")\nelse:\n    print(f\"Directory '{REPO_DIR}' already exists. Pulling latest changesâ€¦\")\n    # If your origin remote wasnâ€™t set with the token, you could uncomment:\n    # os.system(f\"git -C {REPO_DIR} remote set-url origin {REPO_URL}\")\n    os.system(f\"git -C {REPO_DIR} pull\")\n\n# 3) Change into the repo directory\nos.chdir(REPO_DIR)\nprint(\"Current working directory:\", os.getcwd())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T23:34:04.381502Z","iopub.execute_input":"2025-05-06T23:34:04.381778Z","iopub.status.idle":"2025-05-06T23:34:05.369305Z","shell.execute_reply.started":"2025-05-06T23:34:04.381750Z","shell.execute_reply":"2025-05-06T23:34:05.368550Z"}},"outputs":[{"name":"stdout","text":"Cloning repository into ./BreastCATTâ€¦\n","output_type":"stream"},{"name":"stderr","text":"Cloning into 'BreastCATT'...\n","output_type":"stream"},{"name":"stdout","text":"Current working directory: /kaggle/working/BreastCATT\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!git fetch origin\n!git pull origin main\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T23:45:46.487515Z","iopub.execute_input":"2025-05-06T23:45:46.487844Z","iopub.status.idle":"2025-05-06T23:45:47.591373Z","shell.execute_reply.started":"2025-05-06T23:45:46.487814Z","shell.execute_reply":"2025-05-06T23:45:47.590656Z"}},"outputs":[{"name":"stdout","text":"remote: Enumerating objects: 5, done.\u001b[K\nremote: Counting objects: 100% (5/5), done.\u001b[K\nremote: Compressing objects: 100% (1/1), done.\u001b[K\nremote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0 (from 0)\u001b[K\nUnpacking objects: 100% (3/3), 354 bytes | 177.00 KiB/s, done.\nFrom https://github.com/semilleroCV/BreastCATT\n   89760d5..ed9a6da  main       -> origin/main\nFrom https://github.com/semilleroCV/BreastCATT\n * branch            main       -> FETCH_HEAD\nUpdating 89760d5..ed9a6da\nFast-forward\n finetune.py | 3 \u001b[32m+\u001b[m\u001b[31m--\u001b[m\n 1 file changed, 1 insertion(+), 2 deletions(-)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# every time you do a factory reset you have to run this\n!pip install -r requirements.txt -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T23:34:05.371057Z","iopub.execute_input":"2025-05-06T23:34:05.371595Z","iopub.status.idle":"2025-05-06T23:36:06.680273Z","shell.execute_reply.started":"2025-05-06T23:34:05.371566Z","shell.execute_reply":"2025-05-06T23:36:06.679510Z"}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# log in hugging face hug to load data and models\nfrom huggingface_hub import login\n\nlogin(token=\"\", add_to_git_credential=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T23:41:56.762853Z","iopub.execute_input":"2025-05-06T23:41:56.763429Z","iopub.status.idle":"2025-05-06T23:41:56.957350Z","shell.execute_reply.started":"2025-05-06T23:41:56.763399Z","shell.execute_reply":"2025-05-06T23:41:56.956620Z"}},"outputs":[{"name":"stderr","text":"Token has not been saved to git credential helper.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_ENTITY\"] = \"ai-uis\" \n\n# log in wand to track the experiments\nimport wandb\n\nwandb.login(key=\"\", relogin=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T23:42:00.121371Z","iopub.execute_input":"2025-05-06T23:42:00.121977Z","iopub.status.idle":"2025-05-06T23:42:00.217882Z","shell.execute_reply.started":"2025-05-06T23:42:00.121951Z","shell.execute_reply":"2025-05-06T23:42:00.217117Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"from accelerate.utils import write_basic_config\n\n# Configure accelerate\nwrite_basic_config(mixed_precision=\"fp16\")\n!cat /root/.cache/huggingface/accelerate/default_config.yaml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T23:36:14.894405Z","iopub.execute_input":"2025-05-06T23:36:14.894759Z","iopub.status.idle":"2025-05-06T23:36:19.257331Z","shell.execute_reply.started":"2025-05-06T23:36:14.894736Z","shell.execute_reply":"2025-05-06T23:36:19.256308Z"}},"outputs":[{"name":"stdout","text":"{\n  \"compute_environment\": \"LOCAL_MACHINE\",\n  \"debug\": false,\n  \"distributed_type\": \"MULTI_GPU\",\n  \"downcast_bf16\": false,\n  \"enable_cpu_affinity\": false,\n  \"machine_rank\": 0,\n  \"main_training_function\": \"main\",\n  \"mixed_precision\": \"fp16\",\n  \"num_machines\": 1,\n  \"num_processes\": 2,\n  \"rdzv_backend\": \"static\",\n  \"same_network\": false,\n  \"tpu_use_cluster\": false,\n  \"tpu_use_sudo\": false,\n  \"use_cpu\": false\n}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"! accelerate test\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T23:39:04.602778Z","iopub.execute_input":"2025-05-06T23:39:04.603096Z","iopub.status.idle":"2025-05-06T23:39:21.859450Z","shell.execute_reply.started":"2025-05-06T23:39:04.603067Z","shell.execute_reply":"2025-05-06T23:39:21.858443Z"}},"outputs":[{"name":"stdout","text":"\nRunning:  accelerate-launch /usr/local/lib/python3.11/dist-packages/accelerate/test_utils/scripts/test_script.py\nstdout: **Initialization**\nstdout: Testing, testing. 1, 2, 3.\nstdout: Distributed environment: DistributedType.MULTI_GPU  Backend: nccl\nstdout: Num processes: 2\nstdout: Process index: 0\nstdout: Local process index: 0\nstdout: Device: cuda:0\nstdout: \nstdout: Mixed precision type: fp16\nstdout: \nstderr: [rank0]:[W506 23:39:15.366767166 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\nstdout: Distributed environment: DistributedType.MULTI_GPU  Backend: nccl\nstdout: Num processes: 2\nstdout: Process index: 1\nstdout: Local process index: 1\nstdout: Device: cuda:1\nstdout: \nstdout: Mixed precision type: fp16\nstdout: \nstderr: [rank1]:[W506 23:39:15.376370096 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\nstdout: \nstdout: **Test process execution**\nstdout: \nstdout: **Test split between processes as a list**\nstdout: \nstdout: **Test split between processes as a dict**\nstdout: \nstdout: **Test split between processes as a tensor**\nstdout: \nstdout: **Test split between processes evenly**\nstdout: \nstdout: **Test split between processes as a datasets.Dataset**\nstdout: \nstdout: **Test random number generator synchronization**\nstdout: All rng are properly synched.\nstdout: \nstdout: **DataLoader integration test**\nstdout: Non-shuffled dataloader passing.\nstdout: Shuffled dataloader passing.\nstdout: Non-shuffled central dataloader passing.\nstdout: Shuffled central dataloader passing.\nstdout: \nstdout: **Training integration test**\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Training yielded the same results on one CPU or distributed setup with no batch split.\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Keep fp32 wrapper check.\nstdout: Training yielded the same results on one CPU or distributed setup with batch split.\nstdout: Keep fp32 wrapper check.\nstdout: BF16 training check.\nstdout: BF16 training check.\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: FP16 training check.\nstdout: FP16 training check.\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: \nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: \nstdout: Training yielded the same results on one CPU or distributed setup with no batch split.\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Training yielded the same results on one CPU or distributed setup with batch split.Keep fp32 wrapper check.\nstdout: \nstdout: Keep fp32 wrapper check.\nstdout: BF16 training check.\nstdout: BF16 training check.\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: FP16 training check.\nstdout: FP16 training check.\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\nstdout: \nstdout: **Breakpoint trigger test**\nstdout: \nstdout: **Test reinstantiated state**\nTest is a success! You are ready for your distributed training!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Running finetune script","metadata":{}},{"cell_type":"markdown","source":" change the id of the agent and \"n, i.e. 5, for five runs\"","metadata":{}},{"cell_type":"code","source":"! wandb agent ai-uis/colcaci/bgvdh7rq --count 3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T23:45:55.269010Z","iopub.execute_input":"2025-05-06T23:45:55.269319Z","iopub.status.idle":"2025-05-06T23:48:39.615056Z","shell.execute_reply.started":"2025-05-06T23:45:55.269296Z","shell.execute_reply":"2025-05-06T23:48:39.614299Z"}},"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Starting wandb agent ğŸ•µï¸\n2025-05-06 23:46:01,654 - wandb.wandb_agent - INFO - Running runs: []\n2025-05-06 23:46:02,101 - wandb.wandb_agent - INFO - Agent received command: run\n2025-05-06 23:46:02,101 - wandb.wandb_agent - INFO - Agent starting run with config:\n\tcheckpointing_steps: None\n\tdataset_name: SemilleroCV/DMR-IR\n\tgradient_accumulation_steps: 1\n\tlearning_rate: 7.821934575816042e-05\n\tmax_eval_samples: 50\n\tmax_train_samples: 100\n\tmodel_name_or_path: google/vit-base-patch16-224-in21k\n\tnum_train_epochs: 5\n\tper_device_eval_batch_size: 16\n\tper_device_train_batch_size: 5\n2025-05-06 23:46:02,103 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python finetune.py --checkpointing_steps=None --dataset_name=SemilleroCV/DMR-IR --gradient_accumulation_steps=1 --learning_rate=7.821934575816042e-05 --max_eval_samples=50 --max_train_samples=100 --model_name_or_path=google/vit-base-patch16-224-in21k --num_train_epochs=5 --per_device_eval_batch_size=16 --per_device_train_batch_size=5\n2025-05-06 23:46:06.937121: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746575166.960721     676 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746575166.967891     676 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-05-06 23:46:07,116 - wandb.wandb_agent - INFO - Running runs: ['376113ms']\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/config.json\nModel config ViTConfig {\n  \"architectures\": [\n    \"ViTModel\"\n  ],\n  \"attention_probs_dropout_prob\": 0.0,\n  \"encoder_stride\": 16,\n  \"finetuning_task\": \"image-classification\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"benign\",\n    \"1\": \"malignant\"\n  },\n  \"image_size\": 224,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"benign\": \"0\",\n    \"malignant\": \"1\"\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"model_type\": \"vit\",\n  \"num_attention_heads\": 12,\n  \"num_channels\": 3,\n  \"num_hidden_layers\": 12,\n  \"patch_size\": 16,\n  \"pooler_act\": \"tanh\",\n  \"pooler_output_size\": 768,\n  \"qkv_bias\": true,\n  \"transformers_version\": \"4.52.0.dev0\"\n}\n\nloading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/preprocessor_config.json\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/config.json\nModel config ViTConfig {\n  \"architectures\": [\n    \"ViTModel\"\n  ],\n  \"attention_probs_dropout_prob\": 0.0,\n  \"encoder_stride\": 16,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"image_size\": 224,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"model_type\": \"vit\",\n  \"num_attention_heads\": 12,\n  \"num_channels\": 3,\n  \"num_hidden_layers\": 12,\n  \"patch_size\": 16,\n  \"pooler_act\": \"tanh\",\n  \"pooler_output_size\": 768,\n  \"qkv_bias\": true,\n  \"transformers_version\": \"4.52.0.dev0\"\n}\n\nFast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\nloading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/preprocessor_config.json\nsize should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}, {'max_height', 'max_width'}), got 224. Converted to {'height': 224, 'width': 224}.\nImage processor ViTImageProcessor {\n  \"do_convert_rgb\": null,\n  \"do_normalize\": true,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"image_mean\": [\n    0.5,\n    0.5,\n    0.5\n  ],\n  \"image_processor_type\": \"ViTImageProcessor\",\n  \"image_std\": [\n    0.5,\n    0.5,\n    0.5\n  ],\n  \"resample\": 2,\n  \"rescale_factor\": 0.00392156862745098,\n  \"size\": {\n    \"height\": 224,\n    \"width\": 224\n  }\n}\n\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/model.safetensors\nSome weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mguillepinto\u001b[0m (\u001b[33mai-uis\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'colcaci' when running a sweep.\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/BreastCATT/wandb/run-20250506_234615-376113ms\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfine-sweep-18\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ai-uis/colcaci\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ğŸ§¹ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ai-uis/colcaci/sweeps/bgvdh7rq\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ai-uis/colcaci/runs/376113ms\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dataset_name' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_train_samples' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_eval_samples' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'model_name_or_path' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_eval_batch_size' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'checkpointing_steps' was locked by 'sweep' (ignored update).\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  5.19it/s]\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:    accuracy â–â–ˆâ–ˆâ–ˆâ–ˆ\n\u001b[34m\u001b[1mwandb\u001b[0m:       epoch â–â–ƒâ–…â–†â–ˆ\n\u001b[34m\u001b[1mwandb\u001b[0m:   precision â–ˆâ–â–â–â–\n\u001b[34m\u001b[1mwandb\u001b[0m: sensitivity â–ˆâ–â–â–â–\n\u001b[34m\u001b[1mwandb\u001b[0m: specificity â–â–ˆâ–ˆâ–ˆâ–ˆ\n\u001b[34m\u001b[1mwandb\u001b[0m:        step â–â–ƒâ–…â–†â–ˆ\n\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss â–ˆâ–‡â–‚â–‚â–\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:    accuracy 0.54\n\u001b[34m\u001b[1mwandb\u001b[0m:       epoch 4\n\u001b[34m\u001b[1mwandb\u001b[0m:   precision 0.0\n\u001b[34m\u001b[1mwandb\u001b[0m: sensitivity 0.0\n\u001b[34m\u001b[1mwandb\u001b[0m: specificity 1.0\n\u001b[34m\u001b[1mwandb\u001b[0m:        step 100\n\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss 0.69293\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run \u001b[33mfine-sweep-18\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ai-uis/colcaci/runs/376113ms\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ai-uis/colcaci\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250506_234615-376113ms/logs\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\nConfiguration saved in runs/vit-base-patch16-224-in21k-5bs-5ep-7.821934575816042e-05lr/config.json\nModel weights saved in runs/vit-base-patch16-224-in21k-5bs-5ep-7.821934575816042e-05lr/model.safetensors\nImage processor saved in runs/vit-base-patch16-224-in21k-5bs-5ep-7.821934575816042e-05lr/preprocessor_config.json\n/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py:2310: UserWarning: Run (376113ms) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n  lambda data: self._console_raw_callback(\"stderr\", data),\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:27<00:00,  3.59it/s]\n2025-05-06 23:46:52,499 - wandb.wandb_agent - INFO - Cleaning up finished run: 376113ms\n2025-05-06 23:46:53,116 - wandb.wandb_agent - INFO - Agent received command: run\n2025-05-06 23:46:53,116 - wandb.wandb_agent - INFO - Agent starting run with config:\n\tcheckpointing_steps: None\n\tdataset_name: SemilleroCV/DMR-IR\n\tgradient_accumulation_steps: 1\n\tlearning_rate: 4.434447370263611e-05\n\tmax_eval_samples: 50\n\tmax_train_samples: 100\n\tmodel_name_or_path: google/vit-base-patch16-224-in21k\n\tnum_train_epochs: 5\n\tper_device_eval_batch_size: 16\n\tper_device_train_batch_size: 4\n2025-05-06 23:46:53,118 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python finetune.py --checkpointing_steps=None --dataset_name=SemilleroCV/DMR-IR --gradient_accumulation_steps=1 --learning_rate=4.434447370263611e-05 --max_eval_samples=50 --max_train_samples=100 --model_name_or_path=google/vit-base-patch16-224-in21k --num_train_epochs=5 --per_device_eval_batch_size=16 --per_device_train_batch_size=4\n2025-05-06 23:46:57.895690: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746575217.919689     790 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746575217.926876     790 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-05-06 23:46:58,131 - wandb.wandb_agent - INFO - Running runs: ['n45752hx']\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/config.json\nModel config ViTConfig {\n  \"architectures\": [\n    \"ViTModel\"\n  ],\n  \"attention_probs_dropout_prob\": 0.0,\n  \"encoder_stride\": 16,\n  \"finetuning_task\": \"image-classification\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"benign\",\n    \"1\": \"malignant\"\n  },\n  \"image_size\": 224,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"benign\": \"0\",\n    \"malignant\": \"1\"\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"model_type\": \"vit\",\n  \"num_attention_heads\": 12,\n  \"num_channels\": 3,\n  \"num_hidden_layers\": 12,\n  \"patch_size\": 16,\n  \"pooler_act\": \"tanh\",\n  \"pooler_output_size\": 768,\n  \"qkv_bias\": true,\n  \"transformers_version\": \"4.52.0.dev0\"\n}\n\nloading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/preprocessor_config.json\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/config.json\nModel config ViTConfig {\n  \"architectures\": [\n    \"ViTModel\"\n  ],\n  \"attention_probs_dropout_prob\": 0.0,\n  \"encoder_stride\": 16,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"image_size\": 224,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"model_type\": \"vit\",\n  \"num_attention_heads\": 12,\n  \"num_channels\": 3,\n  \"num_hidden_layers\": 12,\n  \"patch_size\": 16,\n  \"pooler_act\": \"tanh\",\n  \"pooler_output_size\": 768,\n  \"qkv_bias\": true,\n  \"transformers_version\": \"4.52.0.dev0\"\n}\n\nFast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\nloading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/preprocessor_config.json\nsize should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}, {'max_width', 'max_height'}), got 224. Converted to {'height': 224, 'width': 224}.\nImage processor ViTImageProcessor {\n  \"do_convert_rgb\": null,\n  \"do_normalize\": true,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"image_mean\": [\n    0.5,\n    0.5,\n    0.5\n  ],\n  \"image_processor_type\": \"ViTImageProcessor\",\n  \"image_std\": [\n    0.5,\n    0.5,\n    0.5\n  ],\n  \"resample\": 2,\n  \"rescale_factor\": 0.00392156862745098,\n  \"size\": {\n    \"height\": 224,\n    \"width\": 224\n  }\n}\n\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/model.safetensors\nSome weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mguillepinto\u001b[0m (\u001b[33mai-uis\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'colcaci' when running a sweep.\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/BreastCATT/wandb/run-20250506_234707-n45752hx\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdifferent-sweep-19\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ai-uis/colcaci\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ğŸ§¹ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ai-uis/colcaci/sweeps/bgvdh7rq\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ai-uis/colcaci/runs/n45752hx\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dataset_name' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_train_samples' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_eval_samples' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'model_name_or_path' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_eval_batch_size' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'checkpointing_steps' was locked by 'sweep' (ignored update).\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:24<00:00,  6.01it/s]\u001b[34m\u001b[1mwandb\u001b[0m: 0.002 MB of 0.002 MB uploaded\n\u001b[34m\u001b[1mwandb\u001b[0m: 0.002 MB of 0.002 MB uploaded\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: 0.002 MB of 0.002 MB uploaded\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:    accuracy â–ˆâ–ˆâ–ˆâ–ˆâ–\n\u001b[34m\u001b[1mwandb\u001b[0m:       epoch â–â–ƒâ–…â–†â–ˆ\n\u001b[34m\u001b[1mwandb\u001b[0m:   precision â–â–â–â–â–ˆ\n\u001b[34m\u001b[1mwandb\u001b[0m: sensitivity â–â–â–â–â–ˆ\n\u001b[34m\u001b[1mwandb\u001b[0m: specificity â–ˆâ–ˆâ–ˆâ–ˆâ–\n\u001b[34m\u001b[1mwandb\u001b[0m:        step â–â–ƒâ–…â–†â–ˆ\n\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss â–ˆâ–…â–‚â–â–\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:    accuracy 0.52\n\u001b[34m\u001b[1mwandb\u001b[0m:       epoch 4\n\u001b[34m\u001b[1mwandb\u001b[0m:   precision 0.47826\n\u001b[34m\u001b[1mwandb\u001b[0m: sensitivity 0.47826\n\u001b[34m\u001b[1mwandb\u001b[0m: specificity 0.55556\n\u001b[34m\u001b[1mwandb\u001b[0m:        step 125\n\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss 0.69463\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run \u001b[33mdifferent-sweep-19\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ai-uis/colcaci/runs/n45752hx\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ai-uis/colcaci\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250506_234707-n45752hx/logs\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\nConfiguration saved in runs/vit-base-patch16-224-in21k-4bs-5ep-4.434447370263611e-05lr/config.json\nModel weights saved in runs/vit-base-patch16-224-in21k-4bs-5ep-4.434447370263611e-05lr/model.safetensors\nImage processor saved in runs/vit-base-patch16-224-in21k-4bs-5ep-4.434447370263611e-05lr/preprocessor_config.json\n/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py:2310: UserWarning: Run (n45752hx) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n  lambda data: self._console_raw_callback(\"stderr\", data),\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:29<00:00,  4.18it/s]\n2025-05-06 23:47:48,451 - wandb.wandb_agent - INFO - Cleaning up finished run: n45752hx\n2025-05-06 23:47:48,911 - wandb.wandb_agent - INFO - Agent received command: run\n2025-05-06 23:47:48,912 - wandb.wandb_agent - INFO - Agent starting run with config:\n\tcheckpointing_steps: None\n\tdataset_name: SemilleroCV/DMR-IR\n\tgradient_accumulation_steps: 1\n\tlearning_rate: 9.296377435003328e-05\n\tmax_eval_samples: 50\n\tmax_train_samples: 100\n\tmodel_name_or_path: google/vit-base-patch16-224-in21k\n\tnum_train_epochs: 5\n\tper_device_eval_batch_size: 16\n\tper_device_train_batch_size: 4\n2025-05-06 23:47:48,913 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python finetune.py --checkpointing_steps=None --dataset_name=SemilleroCV/DMR-IR --gradient_accumulation_steps=1 --learning_rate=9.296377435003328e-05 --max_eval_samples=50 --max_train_samples=100 --model_name_or_path=google/vit-base-patch16-224-in21k --num_train_epochs=5 --per_device_eval_batch_size=16 --per_device_train_batch_size=4\n2025-05-06 23:47:53.790728: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746575273.814102     904 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746575273.821515     904 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-05-06 23:47:53,926 - wandb.wandb_agent - INFO - Running runs: ['3v6y402i']\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/config.json\nModel config ViTConfig {\n  \"architectures\": [\n    \"ViTModel\"\n  ],\n  \"attention_probs_dropout_prob\": 0.0,\n  \"encoder_stride\": 16,\n  \"finetuning_task\": \"image-classification\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"benign\",\n    \"1\": \"malignant\"\n  },\n  \"image_size\": 224,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"benign\": \"0\",\n    \"malignant\": \"1\"\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"model_type\": \"vit\",\n  \"num_attention_heads\": 12,\n  \"num_channels\": 3,\n  \"num_hidden_layers\": 12,\n  \"patch_size\": 16,\n  \"pooler_act\": \"tanh\",\n  \"pooler_output_size\": 768,\n  \"qkv_bias\": true,\n  \"transformers_version\": \"4.52.0.dev0\"\n}\n\nloading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/preprocessor_config.json\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/config.json\nModel config ViTConfig {\n  \"architectures\": [\n    \"ViTModel\"\n  ],\n  \"attention_probs_dropout_prob\": 0.0,\n  \"encoder_stride\": 16,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"image_size\": 224,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"model_type\": \"vit\",\n  \"num_attention_heads\": 12,\n  \"num_channels\": 3,\n  \"num_hidden_layers\": 12,\n  \"patch_size\": 16,\n  \"pooler_act\": \"tanh\",\n  \"pooler_output_size\": 768,\n  \"qkv_bias\": true,\n  \"transformers_version\": \"4.52.0.dev0\"\n}\n\nFast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\nloading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/preprocessor_config.json\nsize should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}, {'max_width', 'max_height'}), got 224. Converted to {'height': 224, 'width': 224}.\nImage processor ViTImageProcessor {\n  \"do_convert_rgb\": null,\n  \"do_normalize\": true,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"image_mean\": [\n    0.5,\n    0.5,\n    0.5\n  ],\n  \"image_processor_type\": \"ViTImageProcessor\",\n  \"image_std\": [\n    0.5,\n    0.5,\n    0.5\n  ],\n  \"resample\": 2,\n  \"rescale_factor\": 0.00392156862745098,\n  \"size\": {\n    \"height\": 224,\n    \"width\": 224\n  }\n}\n\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0/model.safetensors\nSome weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mguillepinto\u001b[0m (\u001b[33mai-uis\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'colcaci' when running a sweep.\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/BreastCATT/wandb/run-20250506_234801-3v6y402i\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfluent-sweep-20\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ai-uis/colcaci\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ğŸ§¹ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ai-uis/colcaci/sweeps/bgvdh7rq\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ai-uis/colcaci/runs/3v6y402i\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dataset_name' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_train_samples' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_eval_samples' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'model_name_or_path' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_eval_batch_size' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'checkpointing_steps' was locked by 'sweep' (ignored update).\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:25<00:00,  5.96it/s]\u001b[34m\u001b[1mwandb\u001b[0m: 0.002 MB of 0.002 MB uploaded\n\u001b[34m\u001b[1mwandb\u001b[0m: 0.002 MB of 0.002 MB uploaded\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: 0.002 MB of 0.002 MB uploaded\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:    accuracy â–â–â–â–â–\n\u001b[34m\u001b[1mwandb\u001b[0m:       epoch â–â–ƒâ–…â–†â–ˆ\n\u001b[34m\u001b[1mwandb\u001b[0m:   precision â–â–â–â–â–\n\u001b[34m\u001b[1mwandb\u001b[0m: sensitivity â–â–â–â–â–\n\u001b[34m\u001b[1mwandb\u001b[0m: specificity â–â–â–â–â–\n\u001b[34m\u001b[1mwandb\u001b[0m:        step â–â–ƒâ–…â–†â–ˆ\n\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss â–‡â–ˆâ–ƒâ–â–\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:    accuracy 0.54\n\u001b[34m\u001b[1mwandb\u001b[0m:       epoch 4\n\u001b[34m\u001b[1mwandb\u001b[0m:   precision 0.0\n\u001b[34m\u001b[1mwandb\u001b[0m: sensitivity 0.0\n\u001b[34m\u001b[1mwandb\u001b[0m: specificity 1.0\n\u001b[34m\u001b[1mwandb\u001b[0m:        step 125\n\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss 0.69464\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run \u001b[33mfluent-sweep-20\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ai-uis/colcaci/runs/3v6y402i\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ai-uis/colcaci\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250506_234801-3v6y402i/logs\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The legacy backend is deprecated. In future versions, `wandb-core` will become the sole backend service, and the `wandb.require('legacy-service')` flag will be removed. For more information, visit https://wandb.me/wandb-core\nConfiguration saved in runs/vit-base-patch16-224-in21k-4bs-5ep-9.296377435003328e-05lr/config.json\nModel weights saved in runs/vit-base-patch16-224-in21k-4bs-5ep-9.296377435003328e-05lr/model.safetensors\nImage processor saved in runs/vit-base-patch16-224-in21k-4bs-5ep-9.296377435003328e-05lr/preprocessor_config.json\n/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py:2310: UserWarning: Run (3v6y402i) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n  lambda data: self._console_raw_callback(\"stderr\", data),\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:29<00:00,  4.17it/s]\n2025-05-06 23:48:39,266 - wandb.wandb_agent - INFO - Cleaning up finished run: 3v6y402i\n\u001b[34m\u001b[1mwandb\u001b[0m: Terminating and syncing runs. Press ctrl-c to kill.\n","output_type":"stream"}],"execution_count":14}]}