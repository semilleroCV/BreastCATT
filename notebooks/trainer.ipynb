{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-08T14:59:03.354397Z",
     "iopub.status.busy": "2025-05-08T14:59:03.354120Z",
     "iopub.status.idle": "2025-05-08T14:59:04.416272Z",
     "shell.execute_reply": "2025-05-08T14:59:04.415442Z",
     "shell.execute_reply.started": "2025-05-08T14:59:03.354374Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning repository into ./BreastCATT…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'BreastCATT'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /kaggle/working/BreastCATT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 1) Define your token, repo URL and local directory name\n",
    "GIT_TOKEN = \"\"\n",
    "REPO_URL  = f\"https://{GIT_TOKEN}@github.com/semilleroCV/BreastCATT.git\"\n",
    "REPO_DIR  = \"BreastCATT\"\n",
    "\n",
    "# 2) Clone or pull depending on whether the folder exists\n",
    "if not os.path.isdir(REPO_DIR):\n",
    "    print(f\"Cloning repository into ./{REPO_DIR}…\")\n",
    "    os.system(f\"git clone {REPO_URL}\")\n",
    "else:\n",
    "    print(f\"Directory '{REPO_DIR}' already exists. Pulling latest changes…\")\n",
    "    # If your origin remote wasn’t set with the token, you could uncomment:\n",
    "    # os.system(f\"git -C {REPO_DIR} remote set-url origin {REPO_URL}\")\n",
    "    os.system(f\"git -C {REPO_DIR} pull\")\n",
    "\n",
    "# 3) Change into the repo directory\n",
    "os.chdir(REPO_DIR)\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:59:04.417983Z",
     "iopub.status.busy": "2025-05-08T14:59:04.417692Z",
     "iopub.status.idle": "2025-05-08T14:59:05.325808Z",
     "shell.execute_reply": "2025-05-08T14:59:05.324922Z",
     "shell.execute_reply.started": "2025-05-08T14:59:04.417964Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From https://github.com/semilleroCV/BreastCATT\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "!git fetch origin\n",
    "!git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T14:59:05.326927Z",
     "iopub.status.busy": "2025-05-08T14:59:05.326705Z",
     "iopub.status.idle": "2025-05-08T15:01:01.075988Z",
     "shell.execute_reply": "2025-05-08T15:01:01.075238Z",
     "shell.execute_reply.started": "2025-05-08T14:59:05.326906Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# every time you do a factory reset you have to run this\n",
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T15:01:01.077114Z",
     "iopub.status.busy": "2025-05-08T15:01:01.076904Z",
     "iopub.status.idle": "2025-05-08T15:01:01.824233Z",
     "shell.execute_reply": "2025-05-08T15:01:01.823668Z",
     "shell.execute_reply.started": "2025-05-08T15:01:01.077094Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# log in hugging face hug to load data and models\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"\", add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T15:01:01.826538Z",
     "iopub.status.busy": "2025-05-08T15:01:01.826311Z",
     "iopub.status.idle": "2025-05-08T15:01:09.676691Z",
     "shell.execute_reply": "2025-05-08T15:01:09.676074Z",
     "shell.execute_reply.started": "2025-05-08T15:01:01.826519Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbrayan2221707\u001b[0m (\u001b[33mai-uis\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_ENTITY\"] = \"ai-uis\" \n",
    "\n",
    "# log in wand to track the experiments\n",
    "import wandb\n",
    "\n",
    "wandb.login(key=\"\", relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T15:01:09.677676Z",
     "iopub.status.busy": "2025-05-08T15:01:09.677333Z",
     "iopub.status.idle": "2025-05-08T15:01:13.759672Z",
     "shell.execute_reply": "2025-05-08T15:01:13.758870Z",
     "shell.execute_reply.started": "2025-05-08T15:01:09.677657Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"compute_environment\": \"LOCAL_MACHINE\",\n",
      "  \"debug\": false,\n",
      "  \"distributed_type\": \"MULTI_GPU\",\n",
      "  \"downcast_bf16\": false,\n",
      "  \"enable_cpu_affinity\": false,\n",
      "  \"machine_rank\": 0,\n",
      "  \"main_training_function\": \"main\",\n",
      "  \"mixed_precision\": \"fp16\",\n",
      "  \"num_machines\": 1,\n",
      "  \"num_processes\": 2,\n",
      "  \"rdzv_backend\": \"static\",\n",
      "  \"same_network\": false,\n",
      "  \"tpu_use_cluster\": false,\n",
      "  \"tpu_use_sudo\": false,\n",
      "  \"use_cpu\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from accelerate.utils import write_basic_config\n",
    "\n",
    "# Configure accelerate\n",
    "write_basic_config(mixed_precision=\"fp16\")\n",
    "!cat /root/.cache/huggingface/accelerate/default_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T15:01:13.761777Z",
     "iopub.status.busy": "2025-05-08T15:01:13.760871Z",
     "iopub.status.idle": "2025-05-08T15:01:34.713840Z",
     "shell.execute_reply": "2025-05-08T15:01:34.712913Z",
     "shell.execute_reply.started": "2025-05-08T15:01:13.761751Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running:  accelerate-launch /usr/local/lib/python3.11/dist-packages/accelerate/test_utils/scripts/test_script.py\n",
      "stdout: **Initialization**\n",
      "stdout: Testing, testing. 1, 2, 3.\n",
      "stdout: Distributed environment: DistributedType.MULTI_GPU  Backend: nccl\n",
      "stdout: Num processes: 2\n",
      "stdout: Process index: 0\n",
      "stdout: Local process index: 0\n",
      "stdout: Device: cuda:0\n",
      "stdout: \n",
      "stdout: Mixed precision type: fp16\n",
      "stdout: \n",
      "stdout: Distributed environment: DistributedType.MULTI_GPU  Backend: nccl\n",
      "stdout: Num processes: 2\n",
      "stdout: Process index: 1\n",
      "stdout: Local process index: 1\n",
      "stdout: Device: cuda:1\n",
      "stdout: \n",
      "stdout: Mixed precision type: fp16\n",
      "stdout: \n",
      "stderr: [rank1]:[W508 15:01:26.991873028 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "stderr: [rank0]:[W508 15:01:26.002360450 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "stdout: \n",
      "stdout: **Test process execution**\n",
      "stdout: \n",
      "stdout: **Test split between processes as a list**\n",
      "stdout: \n",
      "stdout: **Test split between processes as a dict**\n",
      "stdout: \n",
      "stdout: **Test split between processes as a tensor**\n",
      "stdout: \n",
      "stdout: **Test split between processes evenly**\n",
      "stdout: \n",
      "stdout: **Test split between processes as a datasets.Dataset**\n",
      "stdout: \n",
      "stdout: **Test random number generator synchronization**\n",
      "stdout: All rng are properly synched.\n",
      "stdout: \n",
      "stdout: **DataLoader integration test**\n",
      "stdout: Non-shuffled dataloader passing.\n",
      "stdout: Shuffled dataloader passing.\n",
      "stdout: Non-shuffled central dataloader passing.\n",
      "stdout: Shuffled central dataloader passing.\n",
      "stdout: \n",
      "stdout: **Training integration test**\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Training yielded the same results on one CPU or distributed setup with no batch split.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Training yielded the same results on one CPU or distributed setup with batch split.\n",
      "stdout: Keep fp32 wrapper check.\n",
      "stdout: Keep fp32 wrapper check.\n",
      "stdout: BF16 training check.\n",
      "stdout: BF16 training check.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: FP16 training check.\n",
      "stdout: FP16 training check.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Training yielded the same results on one CPU or distributed setup with no batch split.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Training yielded the same results on one CPU or distributed setup with batch split.\n",
      "stdout: Keep fp32 wrapper check.\n",
      "stdout: Keep fp32 wrapper check.\n",
      "stdout: BF16 training check.\n",
      "stdout: BF16 training check.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: FP16 training check.\n",
      "stdout: FP16 training check.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: \n",
      "stdout: **Breakpoint trigger test**\n",
      "stdout: \n",
      "stdout: **Test reinstantiated state**\n",
      "Test is a success! You are ready for your distributed training!\n"
     ]
    }
   ],
   "source": [
    "! accelerate test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running finetune script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " change the id of the agent and \"n, i.e. 5, for five runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T15:01:34.715222Z",
     "iopub.status.busy": "2025-05-08T15:01:34.714964Z",
     "iopub.status.idle": "2025-05-08T15:01:36.732066Z",
     "shell.execute_reply": "2025-05-08T15:01:36.731205Z",
     "shell.execute_reply.started": "2025-05-08T15:01:34.715195Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af23ee9ecce94e63920c0e9bf28013ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lucky-sweep-6_0.4937.pth:   0%|          | 0.00/421M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "# Local directory where you want the checkpoint saved\n",
    "save_path = \"/kaggle/working/BreastCATT/checkpoints/segmentation\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Download the file\n",
    "checkpoint_path = hf_hub_download(\n",
    "    repo_id=\"SemilleroCV/transunet-breast-cancer\",\n",
    "    filename=\"lucky-sweep-6_0.4937.pth\",\n",
    "    local_dir=save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T15:01:36.733385Z",
     "iopub.status.busy": "2025-05-08T15:01:36.733008Z",
     "iopub.status.idle": "2025-05-08T15:01:46.231357Z",
     "shell.execute_reply": "2025-05-08T15:01:46.230476Z",
     "shell.execute_reply.started": "2025-05-08T15:01:36.733359Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80106d79a064887814878dee4b418d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mae_pretrain_vit_base.pth:   0%|          | 0.00/343M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1147813f84604482937e88ae872c2787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mae_pretrain_vit_large.pth:   0%|          | 0.00/1.21G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "# Local directory where you want the checkpoint saved\n",
    "save_path = \"/kaggle/working/BreastCATT/checkpoints/fvit\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Download the file\n",
    "checkpoint_path = hf_hub_download(\n",
    "    repo_id=\"SemilleroCV/mae-checkpoints\",\n",
    "    filename=\"mae_pretrain_vit_base.pth\",\n",
    "    local_dir=save_path\n",
    ")\n",
    "\n",
    "# Download the file\n",
    "checkpoint_path = hf_hub_download(\n",
    "    repo_id=\"SemilleroCV/mae-checkpoints\",\n",
    "    filename=\"mae_pretrain_vit_large.pth\",\n",
    "    local_dir=save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T15:01:46.232561Z",
     "iopub.status.busy": "2025-05-08T15:01:46.232294Z",
     "iopub.status.idle": "2025-05-08T15:04:47.606229Z",
     "shell.execute_reply": "2025-05-08T15:04:47.605018Z",
     "shell.execute_reply.started": "2025-05-08T15:01:46.232536Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.7.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch torchvision torchaudio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T15:04:47.607598Z",
     "iopub.status.busy": "2025-05-08T15:04:47.607294Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Starting wandb agent 🕵️\n",
      "2025-05-08 15:04:54,338 - wandb.wandb_agent - INFO - Running runs: []\n",
      "2025-05-08 15:04:54,584 - wandb.wandb_agent - INFO - Agent received command: run\n",
      "2025-05-08 15:04:54,584 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
      "\tcheckpointing_steps: None\n",
      "\tdataset_name: SemilleroCV/DMR-IR\n",
      "\tlearning_rate: 3.0931271145327e-05\n",
      "\tnum_train_epochs: 30\n",
      "\toutput_dir: runs\n",
      "\tper_device_eval_batch_size: 8\n",
      "\tper_device_train_batch_size: 4\n",
      "\tuse_cross_attn: True\n",
      "\tuse_segmentation: True\n",
      "\tvit_version: base\n",
      "2025-05-08 15:04:54,586 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --checkpointing_steps=None --dataset_name=SemilleroCV/DMR-IR --learning_rate=3.0931271145327e-05 --num_train_epochs=30 --output_dir=runs --per_device_eval_batch_size=8 --per_device_train_batch_size=4 --use_cross_attn=True --use_segmentation=True --vit_version=base\n",
      "2025-05-08 15:04:59,601 - wandb.wandb_agent - INFO - Running runs: ['1fk5g0yq']\n",
      "2025-05-08 15:05:01.209083: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746716701.488982     253 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746716701.558605     253 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "README.md: 100%|███████████████████████████| 6.22k/6.22k [00:00<00:00, 34.2MB/s]\n",
      "train-00000-of-00014.parquet: 100%|███████████| 278M/278M [00:01<00:00, 229MB/s]\n",
      "train-00001-of-00014.parquet: 100%|███████████| 274M/274M [00:01<00:00, 223MB/s]\n",
      "train-00002-of-00014.parquet: 100%|███████████| 260M/260M [00:01<00:00, 186MB/s]\n",
      "train-00003-of-00014.parquet: 100%|███████████| 275M/275M [00:01<00:00, 229MB/s]\n",
      "train-00004-of-00014.parquet: 100%|███████████| 288M/288M [00:01<00:00, 233MB/s]\n",
      "train-00005-of-00014.parquet: 100%|███████████| 293M/293M [00:01<00:00, 214MB/s]\n",
      "train-00006-of-00014.parquet: 100%|███████████| 292M/292M [00:01<00:00, 218MB/s]\n",
      "train-00007-of-00014.parquet: 100%|███████████| 276M/276M [00:01<00:00, 196MB/s]\n",
      "train-00008-of-00014.parquet: 100%|███████████| 261M/261M [00:01<00:00, 204MB/s]\n",
      "train-00009-of-00014.parquet: 100%|███████████| 292M/292M [00:01<00:00, 218MB/s]\n",
      "train-00010-of-00014.parquet: 100%|███████████| 295M/295M [00:02<00:00, 143MB/s]\n",
      "train-00011-of-00014.parquet: 100%|███████████| 324M/324M [00:02<00:00, 148MB/s]\n",
      "train-00012-of-00014.parquet: 100%|███████████| 326M/326M [00:02<00:00, 140MB/s]\n",
      "train-00013-of-00014.parquet: 100%|███████████| 328M/328M [00:01<00:00, 227MB/s]\n",
      "test-00000-of-00004.parquet: 100%|████████████| 249M/249M [00:01<00:00, 197MB/s]\n",
      "test-00001-of-00004.parquet: 100%|████████████| 267M/267M [00:01<00:00, 210MB/s]\n",
      "test-00002-of-00004.parquet: 100%|████████████| 264M/264M [00:01<00:00, 226MB/s]\n",
      "test-00003-of-00004.parquet: 100%|████████████| 298M/298M [00:02<00:00, 140MB/s]\n",
      "Generating train split: 100%|███████| 5663/5663 [00:47<00:00, 119.77 examples/s]\n",
      "Generating test split: 100%|████████| 1470/1470 [00:12<00:00, 114.89 examples/s]\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "config.json: 100%|█████████████████████████████| 534/534 [00:00<00:00, 4.90MB/s]\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--UFNLP--gatortron-base/snapshots/127a8d59fbac7cb9b4510ba403d8a1631630ecf9/config.json\n",
      "Model config MegatronBertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"megatron-bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_type\": \"BertWordPieceCase\",\n",
      "  \"transformers_version\": \"4.52.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50176\n",
      "}\n",
      "\n",
      "vocab.txt: 100%|█████████████████████████████| 379k/379k [00:00<00:00, 9.07MB/s]\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--UFNLP--gatortron-base/snapshots/127a8d59fbac7cb9b4510ba403d8a1631630ecf9/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--UFNLP--gatortron-base/snapshots/127a8d59fbac7cb9b4510ba403d8a1631630ecf9/config.json\n",
      "Model config MegatronBertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"megatron-bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_type\": \"BertWordPieceCase\",\n",
      "  \"transformers_version\": \"4.52.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50176\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--UFNLP--gatortron-base/snapshots/127a8d59fbac7cb9b4510ba403d8a1631630ecf9/config.json\n",
      "Model config MegatronBertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"megatron-bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_type\": \"BertWordPieceCase\",\n",
      "  \"transformers_version\": \"4.52.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50176\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--UFNLP--gatortron-base/snapshots/127a8d59fbac7cb9b4510ba403d8a1631630ecf9/config.json\n",
      "Model config MegatronBertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"megatron-bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_type\": \"BertWordPieceCase\",\n",
      "  \"transformers_version\": \"4.52.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50176\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--UFNLP--gatortron-base/snapshots/127a8d59fbac7cb9b4510ba403d8a1631630ecf9/config.json\n",
      "Model config MegatronBertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"megatron-bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_type\": \"BertWordPieceCase\",\n",
      "  \"transformers_version\": \"4.52.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50176\n",
      "}\n",
      "\n",
      "pytorch_model.bin: 100%|██████████████████████| 713M/713M [00:02<00:00, 257MB/s]\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--UFNLP--gatortron-base/snapshots/127a8d59fbac7cb9b4510ba403d8a1631630ecf9/pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "Safetensors PR exists\n",
      "model.safetensors:  37%|████████              | 262M/713M [00:01<00:02, 212MB/s]Some weights of the model checkpoint at UFNLP/gatortron-base were not used when initializing MegatronBertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing MegatronBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MegatronBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of MegatronBertModel were initialized from the model checkpoint at UFNLP/gatortron-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MegatronBertModel for predictions without further training.\n",
      "model.safetensors: 100%|██████████████████████| 713M/713M [00:03<00:00, 222MB/s]\n",
      "Loading checkpoint from: checkpoints/fvit/mae_pretrain_vit_base.pth\n",
      "Adapting patch_embed.proj.weight from 3 channels to 1 channel...\n",
      "\n",
      "✅ Loaded weights: 148 layers.\n",
      "❌ Not found in model: 0 layers.\n",
      "📋 Details of load_state_dict:\n",
      "_IncompatibleKeys(missing_keys=['cls_token', 'pos_embed', 'language_model.model_lm.embeddings.word_embeddings.weight', 'language_model.model_lm.embeddings.position_embeddings.weight', 'language_model.model_lm.embeddings.token_type_embeddings.weight', 'language_model.model_lm.encoder.layer.0.attention.ln.weight', 'language_model.model_lm.encoder.layer.0.attention.ln.bias', 'language_model.model_lm.encoder.layer.0.attention.self.query.weight', 'language_model.model_lm.encoder.layer.0.attention.self.query.bias', 'language_model.model_lm.encoder.layer.0.attention.self.key.weight', 'language_model.model_lm.encoder.layer.0.attention.self.key.bias', 'language_model.model_lm.encoder.layer.0.attention.self.value.weight', 'language_model.model_lm.encoder.layer.0.attention.self.value.bias', 'language_model.model_lm.encoder.layer.0.attention.output.dense.weight', 'language_model.model_lm.encoder.layer.0.attention.output.dense.bias', 'language_model.model_lm.encoder.layer.0.ln.weight', 'language_model.model_lm.encoder.layer.0.ln.bias', 'language_model.model_lm.encoder.layer.0.intermediate.dense.weight', 'language_model.model_lm.encoder.layer.0.intermediate.dense.bias', 'language_model.model_lm.encoder.layer.0.output.dense.weight', 'language_model.model_lm.encoder.layer.0.output.dense.bias', 'language_model.model_lm.encoder.layer.1.attention.ln.weight', 'language_model.model_lm.encoder.layer.1.attention.ln.bias', 'language_model.model_lm.encoder.layer.1.attention.self.query.weight', 'language_model.model_lm.encoder.layer.1.attention.self.query.bias', 'language_model.model_lm.encoder.layer.1.attention.self.key.weight', 'language_model.model_lm.encoder.layer.1.attention.self.key.bias', 'language_model.model_lm.encoder.layer.1.attention.self.value.weight', 'language_model.model_lm.encoder.layer.1.attention.self.value.bias', 'language_model.model_lm.encoder.layer.1.attention.output.dense.weight', 'language_model.model_lm.encoder.layer.1.attention.output.dense.bias', 'language_model.model_lm.encoder.layer.1.ln.weight', 'language_model.model_lm.encoder.layer.1.ln.bias', 'language_model.model_lm.encoder.layer.1.intermediate.dense.weight', 'language_model.model_lm.encoder.layer.1.intermediate.dense.bias', 'language_model.model_lm.encoder.layer.1.output.dense.weight', 'language_model.model_lm.encoder.layer.1.output.dense.bias', 'language_model.model_lm.encoder.layer.2.attention.ln.weight', 'language_model.model_lm.encoder.layer.2.attention.ln.bias', 'language_model.model_lm.encoder.layer.2.attention.self.query.weight', 'language_model.model_lm.encoder.layer.2.attention.self.query.bias', 'language_model.model_lm.encoder.layer.2.attention.self.key.weight', 'language_model.model_lm.encoder.layer.2.attention.self.key.bias', 'language_model.model_lm.encoder.layer.2.attention.self.value.weight', 'language_model.model_lm.encoder.layer.2.attention.self.value.bias', 'language_model.model_lm.encoder.layer.2.attention.output.dense.weight', 'language_model.model_lm.encoder.layer.2.attention.output.dense.bias', 'language_model.model_lm.encoder.layer.2.ln.weight', 'language_model.model_lm.encoder.layer.2.ln.bias', 'language_model.model_lm.encoder.layer.2.intermediate.dense.weight', 'language_model.model_lm.encoder.layer.2.intermediate.dense.bias', 'language_model.model_lm.encoder.layer.2.output.dense.weight', 'language_model.model_lm.encoder.layer.2.output.dense.bias', 'language_model.model_lm.encoder.layer.3.attention.ln.weight', 'language_model.model_lm.encoder.layer.3.attention.ln.bias', 'language_model.model_lm.encoder.layer.3.attention.self.query.weight', 'language_model.model_lm.encoder.layer.3.attention.self.query.bias', 'language_model.model_lm.encoder.layer.3.attention.self.key.weight', 'language_model.model_lm.encoder.layer.3.attention.self.key.bias', 'language_model.model_lm.encoder.layer.3.attention.self.value.weight', 'language_model.model_lm.encoder.layer.3.attention.self.value.bias', 'language_model.model_lm.encoder.layer.3.attention.output.dense.weight', 'language_model.model_lm.encoder.layer.3.attention.output.dense.bias', 'language_model.model_lm.encoder.layer.3.ln.weight', 'language_model.model_lm.encoder.layer.3.ln.bias', 'language_model.model_lm.encoder.layer.3.intermediate.dense.weight', 'language_model.model_lm.encoder.layer.3.intermediate.dense.bias', 'language_model.model_lm.encoder.layer.3.output.dense.weight', 'language_model.model_lm.encoder.layer.3.output.dense.bias', 'language_model.model_lm.encoder.layer.4.attention.ln.weight', 'language_model.model_lm.encoder.layer.4.attention.ln.bias', 'language_model.model_lm.encoder.layer.4.attention.self.query.weight', 'language_model.model_lm.encoder.layer.4.attention.self.query.bias', 'language_model.model_lm.encoder.layer.4.attention.self.key.weight', 'language_model.model_lm.encoder.layer.4.attention.self.key.bias', 'language_model.model_lm.encoder.layer.4.attention.self.value.weight', 'language_model.model_lm.encoder.layer.4.attention.self.value.bias', 'language_model.model_lm.encoder.layer.4.attention.output.dense.weight', 'language_model.model_lm.encoder.layer.4.attention.output.dense.bias', 'language_model.model_lm.encoder.layer.4.ln.weight', 'language_model.model_lm.encoder.layer.4.ln.bias', 'language_model.model_lm.encoder.layer.4.intermediate.dense.weight', 'language_model.model_lm.encoder.layer.4.intermediate.dense.bias', 'language_model.model_lm.encoder.layer.4.output.dense.weight', 'language_model.model_lm.encoder.layer.4.output.dense.bias', 'language_model.model_lm.encoder.layer.5.attention.ln.weight', 'language_model.model_lm.encoder.layer.5.attention.ln.bias', 'language_model.model_lm.encoder.layer.5.attention.self.query.weight', 'language_model.model_lm.encoder.layer.5.attention.self.query.bias', 'language_model.model_lm.encoder.layer.5.attention.self.key.weight', 'language_model.model_lm.encoder.layer.5.attention.self.key.bias', 'language_model.model_lm.encoder.layer.5.attention.self.value.weight', 'language_model.model_lm.encoder.layer.5.attention.self.value.bias', 'language_model.model_lm.encoder.layer.5.attention.output.dense.weight', 'language_model.model_lm.encoder.layer.5.attention.output.dense.bias', 'language_model.model_lm.encoder.layer.5.ln.weight', 'language_model.model_lm.encoder.layer.5.ln.bias', 'language_model.model_lm.encoder.layer.5.intermediate.dense.weight', 'language_model.model_lm.encoder.layer.5.intermediate.dense.bias', 'language_model.model_lm.encoder.layer.5.output.dense.weight', 'language_model.model_lm.encoder.layer.5.output.dense.bias', 'language_model.model_lm.encoder.layer.6.attention.ln.weight', 'language_model.model_lm.encoder.layer.6.attention.ln.bias', 'language_model.model_lm.encoder.layer.6.attention.self.query.weight', 'language_model.model_lm.encoder.layer.6.attention.self.query.bias', 'language_model.model_lm.encoder.layer.6.attention.self.key.weight', 'language_model.model_lm.encoder.layer.6.attention.self.key.bias', 'language_model.model_lm.encoder.layer.6.attention.self.value.weight', 'language_model.model_lm.encoder.layer.6.attention.self.value.bias', 'language_model.model_lm.encoder.layer.6.attention.output.dense.weight', 'language_model.model_lm.encoder.layer.6.attention.output.dense.bias', 'language_model.model_lm.encoder.layer.6.ln.weight', 'language_model.model_lm.encoder.layer.6.ln.bias', 'language_model.model_lm.encoder.layer.6.intermediate.dense.weight', 'language_model.model_lm.encoder.layer.6.intermediate.dense.bias', 'language_model.model_lm.encoder.layer.6.output.dense.weight', 'language_model.model_lm.encoder.layer.6.output.dense.bias', 'language_model.model_lm.encoder.layer.7.attention.ln.weight', 'language_model.model_lm.encoder.layer.7.attention.ln.bias', 'language_model.model_lm.encoder.layer.7.attention.self.query.weight', 'language_model.model_lm.encoder.layer.7.attention.self.query.bias', 'language_model.model_lm.encoder.layer.7.attention.self.key.weight', 'language_model.model_lm.encoder.layer.7.attention.self.key.bias', 'language_model.model_lm.encoder.layer.7.attention.self.value.weight', 'language_model.model_lm.encoder.layer.7.attention.self.value.bias', 'language_model.model_lm.encoder.layer.7.attention.output.dense.weight', 'language_model.model_lm.encoder.layer.7.attention.output.dense.bias', 'language_model.model_lm.encoder.layer.7.ln.weight', 'language_model.model_lm.encoder.layer.7.ln.bias', 'language_model.model_lm.encoder.layer.7.intermediate.dense.weight', 'language_model.model_lm.encoder.layer.7.intermediate.dense.bias', 'language_model.model_lm.encoder.layer.7.output.dense.weight', 'language_model.model_lm.encoder.layer.7.output.dense.bias', 'language_model.model_lm.encoder.layer.8.attention.ln.weight', 'language_model.model_lm.encoder.layer.8.attention.ln.bias', 'language_model.model_lm.encoder.layer.8.attention.self.query.weight', 'language_model.model_lm.encoder.layer.8.attention.self.query.bias', 'language_model.model_lm.encoder.layer.8.attention.self.key.weight', 'language_model.model_lm.encoder.layer.8.attention.self.key.bias', 'language_model.model_lm.encoder.layer.8.attention.self.value.weight', 'language_model.model_lm.encoder.layer.8.attention.self.value.bias', 'language_model.model_lm.encoder.layer.8.attention.output.dense.weight', 'language_model.model_lm.encoder.layer.8.attention.output.dense.bias', 'language_model.model_lm.encoder.layer.8.ln.weight', 'language_model.model_lm.encoder.layer.8.ln.bias', 'language_model.model_lm.encoder.layer.8.intermediate.dense.weight', 'language_model.model_lm.encoder.layer.8.intermediate.dense.bias', 'language_model.model_lm.encoder.layer.8.output.dense.weight', 'language_model.model_lm.encoder.layer.8.output.dense.bias', 'language_model.model_lm.encoder.layer.9.attention.ln.weight', 'language_model.model_lm.encoder.layer.9.attention.ln.bias', 'language_model.model_lm.encoder.layer.9.attention.self.query.weight', 'language_model.model_lm.encoder.layer.9.attention.self.query.bias', 'language_model.model_lm.encoder.layer.9.attention.self.key.weight', 'language_model.model_lm.encoder.layer.9.attention.self.key.bias', 'language_model.model_lm.encoder.layer.9.attention.self.value.weight', 'language_model.model_lm.encoder.layer.9.attention.self.value.bias', 'language_model.model_lm.encoder.layer.9.attention.output.dense.weight', 'language_model.model_lm.encoder.layer.9.attention.output.dense.bias', 'language_model.model_lm.encoder.layer.9.ln.weight', 'language_model.model_lm.encoder.layer.9.ln.bias', 'language_model.model_lm.encoder.layer.9.intermediate.dense.weight', 'language_model.model_lm.encoder.layer.9.intermediate.dense.bias', 'language_model.model_lm.encoder.layer.9.output.dense.weight', 'language_model.model_lm.encoder.layer.9.output.dense.bias', 'language_model.model_lm.encoder.layer.10.attention.ln.weight', 'language_model.model_lm.encoder.layer.10.attention.ln.bias', 'language_model.model_lm.encoder.layer.10.attention.self.query.weight', 'language_model.model_lm.encoder.layer.10.attention.self.query.bias', 'language_model.model_lm.encoder.layer.10.attention.self.key.weight', 'language_model.model_lm.encoder.layer.10.attention.self.key.bias', 'language_model.model_lm.encoder.layer.10.attention.self.value.weight', 'language_model.model_lm.encoder.layer.10.attention.self.value.bias', 'language_model.model_lm.encoder.layer.10.attention.output.dense.weight', 'language_model.model_lm.encoder.layer.10.attention.output.dense.bias', 'language_model.model_lm.encoder.layer.10.ln.weight', 'language_model.model_lm.encoder.layer.10.ln.bias', 'language_model.model_lm.encoder.layer.10.intermediate.dense.weight', 'language_model.model_lm.encoder.layer.10.intermediate.dense.bias', 'language_model.model_lm.encoder.layer.10.output.dense.weight', 'language_model.model_lm.encoder.layer.10.output.dense.bias', 'language_model.model_lm.encoder.layer.11.attention.ln.weight', 'language_model.model_lm.encoder.layer.11.attention.ln.bias', 'language_model.model_lm.encoder.layer.11.attention.self.query.weight', 'language_model.model_lm.encoder.layer.11.attention.self.query.bias', 'language_model.model_lm.encoder.layer.11.attention.self.key.weight', 'language_model.model_lm.encoder.layer.11.attention.self.key.bias', 'language_model.model_lm.encoder.layer.11.attention.self.value.weight', 'language_model.model_lm.encoder.layer.11.attention.self.value.bias', 'language_model.model_lm.encoder.layer.11.attention.output.dense.weight', 'language_model.model_lm.encoder.layer.11.attention.output.dense.bias', 'language_model.model_lm.encoder.layer.11.ln.weight', 'language_model.model_lm.encoder.layer.11.ln.bias', 'language_model.model_lm.encoder.layer.11.intermediate.dense.weight', 'language_model.model_lm.encoder.layer.11.intermediate.dense.bias', 'language_model.model_lm.encoder.layer.11.output.dense.weight', 'language_model.model_lm.encoder.layer.11.output.dense.bias', 'language_model.model_lm.encoder.layer.12.attention.ln.weight', 'language_model.model_lm.encoder.layer.12.attention.ln.bias', 'language_model.model_lm.encoder.layer.12.attention.self.query.weight', 'language_model.model_lm.encoder.layer.12.attention.self.query.bias', 'language_model.model_lm.encoder.layer.12.attention.self.key.weight', 'language_model.model_lm.encoder.layer.12.attention.self.key.bias', 'language_model.model_lm.encoder.layer.12.attention.self.value.weight', 'language_model.model_lm.encoder.layer.12.attention.self.value.bias', 'language_model.model_lm.encoder.layer.12.attention.output.dense.weight', 'language_model.model_lm.encoder.layer.12.attention.output.dense.bias', 'language_model.model_lm.encoder.layer.12.ln.weight', 'language_model.model_lm.encoder.layer.12.ln.bias', 'language_model.model_lm.encoder.layer.12.intermediate.dense.weight', 'language_model.model_lm.encoder.layer.12.intermediate.dense.bias', 'language_model.model_lm.encoder.layer.12.output.dense.weight', 'language_model.model_lm.encoder.layer.12.output.dense.bias', 'language_model.model_lm.encoder.layer.13.attention.ln.weight', 'language_model.model_lm.encoder.layer.13.attention.ln.bias', 'language_model.model_lm.encoder.layer.13.attention.self.query.weight', 'language_model.model_lm.encoder.layer.13.attention.self.query.bias', 'language_model.model_lm.encoder.layer.13.attention.self.key.weight', 'language_model.model_lm.encoder.layer.13.attention.self.key.bias', 'language_model.model_lm.encoder.layer.13.attention.self.value.weight', 'language_model.model_lm.encoder.layer.13.attention.self.value.bias', 'language_model.model_lm.encoder.layer.13.attention.output.dense.weight', 'language_model.model_lm.encoder.layer.13.attention.output.dense.bias', 'language_model.model_lm.encoder.layer.13.ln.weight', 'language_model.model_lm.encoder.layer.13.ln.bias', 'language_model.model_lm.encoder.layer.13.intermediate.dense.weight', 'language_model.model_lm.encoder.layer.13.intermediate.dense.bias', 'language_model.model_lm.encoder.layer.13.output.dense.weight', 'language_model.model_lm.encoder.layer.13.output.dense.bias', 'language_model.model_lm.encoder.layer.14.attention.ln.weight', 'language_model.model_lm.encoder.layer.14.attention.ln.bias', 'language_model.model_lm.encoder.layer.14.attention.self.query.weight', 'language_model.model_lm.encoder.layer.14.attention.self.query.bias', 'language_model.model_lm.encoder.layer.14.attention.self.key.weight', 'language_model.model_lm.encoder.layer.14.attention.self.key.bias', 'language_model.model_lm.encoder.layer.14.attention.self.value.weight', 'language_model.model_lm.encoder.layer.14.attention.self.value.bias', 'language_model.model_lm.encoder.layer.14.attention.output.dense.weight', 'language_model.model_lm.encoder.layer.14.attention.output.dense.bias', 'language_model.model_lm.encoder.layer.14.ln.weight', 'language_model.model_lm.encoder.layer.14.ln.bias', 'language_model.model_lm.encoder.layer.14.intermediate.dense.weight', 'language_model.model_lm.encoder.layer.14.intermediate.dense.bias', 'language_model.model_lm.encoder.layer.14.output.dense.weight', 'language_model.model_lm.encoder.layer.14.output.dense.bias', 'language_model.model_lm.encoder.layer.15.attention.ln.weight', 'language_model.model_lm.encoder.layer.15.attention.ln.bias', 'language_model.model_lm.encoder.layer.15.attention.self.query.weight', 'language_model.model_lm.encoder.layer.15.attention.self.query.bias', 'language_model.model_lm.encoder.layer.15.attention.self.key.weight', 'language_model.model_lm.encoder.layer.15.attention.self.key.bias', 'language_model.model_lm.encoder.layer.15.attention.self.value.weight', 'language_model.model_lm.encoder.layer.15.attention.self.value.bias', 'language_model.model_lm.encoder.layer.15.attention.output.dense.weight', 'language_model.model_lm.encoder.layer.15.attention.output.dense.bias', 'language_model.model_lm.encoder.layer.15.ln.weight', 'language_model.model_lm.encoder.layer.15.ln.bias', 'language_model.model_lm.encoder.layer.15.intermediate.dense.weight', 'language_model.model_lm.encoder.layer.15.intermediate.dense.bias', 'language_model.model_lm.encoder.layer.15.output.dense.weight', 'language_model.model_lm.encoder.layer.15.output.dense.bias', 'language_model.model_lm.encoder.layer.16.attention.ln.weight', 'language_model.model_lm.encoder.layer.16.attention.ln.bias', 'language_model.model_lm.encoder.layer.16.attention.self.query.weight', 'language_model.model_lm.encoder.layer.16.attention.self.query.bias', 'language_model.model_lm.encoder.layer.16.attention.self.key.weight', 'language_model.model_lm.encoder.layer.16.attention.self.key.bias', 'language_model.model_lm.encoder.layer.16.attention.self.value.weight', 'language_model.model_lm.encoder.layer.16.attention.self.value.bias', 'language_model.model_lm.encoder.layer.16.attention.output.dense.weight', 'language_model.model_lm.encoder.layer.16.attention.output.dense.bias', 'language_model.model_lm.encoder.layer.16.ln.weight', 'language_model.model_lm.encoder.layer.16.ln.bias', 'language_model.model_lm.encoder.layer.16.intermediate.dense.weight', 'language_model.model_lm.encoder.layer.16.intermediate.dense.bias', 'language_model.model_lm.encoder.layer.16.output.dense.weight', 'language_model.model_lm.encoder.layer.16.output.dense.bias', 'language_model.model_lm.encoder.layer.17.attention.ln.weight', 'language_model.model_lm.encoder.layer.17.attention.ln.bias', 'language_model.model_lm.encoder.layer.17.attention.self.query.weight', 'language_model.model_lm.encoder.layer.17.attention.self.query.bias', 'language_model.model_lm.encoder.layer.17.attention.self.key.weight', 'language_model.model_lm.encoder.layer.17.attention.self.key.bias', 'language_model.model_lm.encoder.layer.17.attention.self.value.weight', 'language_model.model_lm.encoder.layer.17.attention.self.value.bias', 'language_model.model_lm.encoder.layer.17.attention.output.dense.weight', 'language_model.model_lm.encoder.layer.17.attention.output.dense.bias', 'language_model.model_lm.encoder.layer.17.ln.weight', 'language_model.model_lm.encoder.layer.17.ln.bias', 'language_model.model_lm.encoder.layer.17.intermediate.dense.weight', 'language_model.model_lm.encoder.layer.17.intermediate.dense.bias', 'language_model.model_lm.encoder.layer.17.output.dense.weight', 'language_model.model_lm.encoder.layer.17.output.dense.bias', 'language_model.model_lm.encoder.layer.18.attention.ln.weight', 'language_model.model_lm.encoder.layer.18.attention.ln.bias', 'language_model.model_lm.encoder.layer.18.attention.self.query.weight', 'language_model.model_lm.encoder.layer.18.attention.self.query.bias', 'language_model.model_lm.encoder.layer.18.attention.self.key.weight', 'language_model.model_lm.encoder.layer.18.attention.self.key.bias', 'language_model.model_lm.encoder.layer.18.attention.self.value.weight', 'language_model.model_lm.encoder.layer.18.attention.self.value.bias', 'language_model.model_lm.encoder.layer.18.attention.output.dense.weight', 'language_model.model_lm.encoder.layer.18.attention.output.dense.bias', 'language_model.model_lm.encoder.layer.18.ln.weight', 'language_model.model_lm.encoder.layer.18.ln.bias', 'language_model.model_lm.encoder.layer.18.intermediate.dense.weight', 'language_model.model_lm.encoder.layer.18.intermediate.dense.bias', 'language_model.model_lm.encoder.layer.18.output.dense.weight', 'language_model.model_lm.encoder.layer.18.output.dense.bias', 'language_model.model_lm.encoder.layer.19.attention.ln.weight', 'language_model.model_lm.encoder.layer.19.attention.ln.bias', 'language_model.model_lm.encoder.layer.19.attention.self.query.weight', 'language_model.model_lm.encoder.layer.19.attention.self.query.bias', 'language_model.model_lm.encoder.layer.19.attention.self.key.weight', 'language_model.model_lm.encoder.layer.19.attention.self.key.bias', 'language_model.model_lm.encoder.layer.19.attention.self.value.weight', 'language_model.model_lm.encoder.layer.19.attention.self.value.bias', 'language_model.model_lm.encoder.layer.19.attention.output.dense.weight', 'language_model.model_lm.encoder.layer.19.attention.output.dense.bias', 'language_model.model_lm.encoder.layer.19.ln.weight', 'language_model.model_lm.encoder.layer.19.ln.bias', 'language_model.model_lm.encoder.layer.19.intermediate.dense.weight', 'language_model.model_lm.encoder.layer.19.intermediate.dense.bias', 'language_model.model_lm.encoder.layer.19.output.dense.weight', 'language_model.model_lm.encoder.layer.19.output.dense.bias', 'language_model.model_lm.encoder.layer.20.attention.ln.weight', 'language_model.model_lm.encoder.layer.20.attention.ln.bias', 'language_model.model_lm.encoder.layer.20.attention.self.query.weight', 'language_model.model_lm.encoder.layer.20.attention.self.query.bias', 'language_model.model_lm.encoder.layer.20.attention.self.key.weight', 'language_model.model_lm.encoder.layer.20.attention.self.key.bias', 'language_model.model_lm.encoder.layer.20.attention.self.value.weight', 'language_model.model_lm.encoder.layer.20.attention.self.value.bias', 'language_model.model_lm.encoder.layer.20.attention.output.dense.weight', 'language_model.model_lm.encoder.layer.20.attention.output.dense.bias', 'language_model.model_lm.encoder.layer.20.ln.weight', 'language_model.model_lm.encoder.layer.20.ln.bias', 'language_model.model_lm.encoder.layer.20.intermediate.dense.weight', 'language_model.model_lm.encoder.layer.20.intermediate.dense.bias', 'language_model.model_lm.encoder.layer.20.output.dense.weight', 'language_model.model_lm.encoder.layer.20.output.dense.bias', 'language_model.model_lm.encoder.layer.21.attention.ln.weight', 'language_model.model_lm.encoder.layer.21.attention.ln.bias', 'language_model.model_lm.encoder.layer.21.attention.self.query.weight', 'language_model.model_lm.encoder.layer.21.attention.self.query.bias', 'language_model.model_lm.encoder.layer.21.attention.self.key.weight', 'language_model.model_lm.encoder.layer.21.attention.self.key.bias', 'language_model.model_lm.encoder.layer.21.attention.self.value.weight', 'language_model.model_lm.encoder.layer.21.attention.self.value.bias', 'language_model.model_lm.encoder.layer.21.attention.output.dense.weight', 'language_model.model_lm.encoder.layer.21.attention.output.dense.bias', 'language_model.model_lm.encoder.layer.21.ln.weight', 'language_model.model_lm.encoder.layer.21.ln.bias', 'language_model.model_lm.encoder.layer.21.intermediate.dense.weight', 'language_model.model_lm.encoder.layer.21.intermediate.dense.bias', 'language_model.model_lm.encoder.layer.21.output.dense.weight', 'language_model.model_lm.encoder.layer.21.output.dense.bias', 'language_model.model_lm.encoder.layer.22.attention.ln.weight', 'language_model.model_lm.encoder.layer.22.attention.ln.bias', 'language_model.model_lm.encoder.layer.22.attention.self.query.weight', 'language_model.model_lm.encoder.layer.22.attention.self.query.bias', 'language_model.model_lm.encoder.layer.22.attention.self.key.weight', 'language_model.model_lm.encoder.layer.22.attention.self.key.bias', 'language_model.model_lm.encoder.layer.22.attention.self.value.weight', 'language_model.model_lm.encoder.layer.22.attention.self.value.bias', 'language_model.model_lm.encoder.layer.22.attention.output.dense.weight', 'language_model.model_lm.encoder.layer.22.attention.output.dense.bias', 'language_model.model_lm.encoder.layer.22.ln.weight', 'language_model.model_lm.encoder.layer.22.ln.bias', 'language_model.model_lm.encoder.layer.22.intermediate.dense.weight', 'language_model.model_lm.encoder.layer.22.intermediate.dense.bias', 'language_model.model_lm.encoder.layer.22.output.dense.weight', 'language_model.model_lm.encoder.layer.22.output.dense.bias', 'language_model.model_lm.encoder.layer.23.attention.ln.weight', 'language_model.model_lm.encoder.layer.23.attention.ln.bias', 'language_model.model_lm.encoder.layer.23.attention.self.query.weight', 'language_model.model_lm.encoder.layer.23.attention.self.query.bias', 'language_model.model_lm.encoder.layer.23.attention.self.key.weight', 'language_model.model_lm.encoder.layer.23.attention.self.key.bias', 'language_model.model_lm.encoder.layer.23.attention.self.value.weight', 'language_model.model_lm.encoder.layer.23.attention.self.value.bias', 'language_model.model_lm.encoder.layer.23.attention.output.dense.weight', 'language_model.model_lm.encoder.layer.23.attention.output.dense.bias', 'language_model.model_lm.encoder.layer.23.ln.weight', 'language_model.model_lm.encoder.layer.23.ln.bias', 'language_model.model_lm.encoder.layer.23.intermediate.dense.weight', 'language_model.model_lm.encoder.layer.23.intermediate.dense.bias', 'language_model.model_lm.encoder.layer.23.output.dense.weight', 'language_model.model_lm.encoder.layer.23.output.dense.bias', 'language_model.model_lm.encoder.ln.weight', 'language_model.model_lm.encoder.ln.bias', 'language_model.model_lm.pooler.dense.weight', 'language_model.model_lm.pooler.dense.bias', 'language_model.proj.0.weight', 'language_model.proj.0.bias', 'language_model.proj.2.weight', 'language_model.proj.2.bias', 'segmentation_model.model.transformer.embeddings.position_embeddings', 'segmentation_model.model.transformer.embeddings.hybrid_model.root.conv.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.root.gn.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.root.gn.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit1.gn1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit1.gn1.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit1.conv1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit1.gn2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit1.gn2.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit1.conv2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit1.gn3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit1.gn3.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit1.conv3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit1.downsample.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit1.gn_proj.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit1.gn_proj.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit2.gn1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit2.gn1.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit2.conv1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit2.gn2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit2.gn2.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit2.conv2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit2.gn3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit2.gn3.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit2.conv3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit3.gn1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit3.gn1.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit3.conv1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit3.gn2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit3.gn2.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit3.conv2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit3.gn3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit3.gn3.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block1.unit3.conv3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit1.gn1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit1.gn1.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit1.conv1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit1.gn2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit1.gn2.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit1.conv2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit1.gn3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit1.gn3.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit1.conv3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit1.downsample.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit1.gn_proj.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit1.gn_proj.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit2.gn1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit2.gn1.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit2.conv1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit2.gn2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit2.gn2.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit2.conv2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit2.gn3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit2.gn3.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit2.conv3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit3.gn1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit3.gn1.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit3.conv1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit3.gn2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit3.gn2.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit3.conv2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit3.gn3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit3.gn3.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit3.conv3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit4.gn1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit4.gn1.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit4.conv1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit4.gn2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit4.gn2.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit4.conv2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit4.gn3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit4.gn3.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block2.unit4.conv3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit1.gn1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit1.gn1.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit1.conv1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit1.gn2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit1.gn2.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit1.conv2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit1.gn3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit1.gn3.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit1.conv3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit1.downsample.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit1.gn_proj.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit1.gn_proj.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit2.gn1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit2.gn1.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit2.conv1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit2.gn2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit2.gn2.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit2.conv2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit2.gn3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit2.gn3.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit2.conv3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit3.gn1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit3.gn1.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit3.conv1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit3.gn2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit3.gn2.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit3.conv2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit3.gn3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit3.gn3.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit3.conv3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit4.gn1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit4.gn1.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit4.conv1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit4.gn2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit4.gn2.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit4.conv2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit4.gn3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit4.gn3.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit4.conv3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit5.gn1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit5.gn1.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit5.conv1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit5.gn2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit5.gn2.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit5.conv2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit5.gn3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit5.gn3.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit5.conv3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit6.gn1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit6.gn1.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit6.conv1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit6.gn2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit6.gn2.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit6.conv2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit6.gn3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit6.gn3.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit6.conv3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit7.gn1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit7.gn1.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit7.conv1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit7.gn2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit7.gn2.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit7.conv2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit7.gn3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit7.gn3.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit7.conv3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit8.gn1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit8.gn1.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit8.conv1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit8.gn2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit8.gn2.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit8.conv2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit8.gn3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit8.gn3.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit8.conv3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit9.gn1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit9.gn1.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit9.conv1.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit9.gn2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit9.gn2.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit9.conv2.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit9.gn3.weight', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit9.gn3.bias', 'segmentation_model.model.transformer.embeddings.hybrid_model.body.block3.unit9.conv3.weight', 'segmentation_model.model.transformer.embeddings.patch_embeddings.weight', 'segmentation_model.model.transformer.embeddings.patch_embeddings.bias', 'segmentation_model.model.transformer.encoder.layer.0.attention_norm.weight', 'segmentation_model.model.transformer.encoder.layer.0.attention_norm.bias', 'segmentation_model.model.transformer.encoder.layer.0.ffn_norm.weight', 'segmentation_model.model.transformer.encoder.layer.0.ffn_norm.bias', 'segmentation_model.model.transformer.encoder.layer.0.ffn.fc1.weight', 'segmentation_model.model.transformer.encoder.layer.0.ffn.fc1.bias', 'segmentation_model.model.transformer.encoder.layer.0.ffn.fc2.weight', 'segmentation_model.model.transformer.encoder.layer.0.ffn.fc2.bias', 'segmentation_model.model.transformer.encoder.layer.0.attn.query.weight', 'segmentation_model.model.transformer.encoder.layer.0.attn.query.bias', 'segmentation_model.model.transformer.encoder.layer.0.attn.key.weight', 'segmentation_model.model.transformer.encoder.layer.0.attn.key.bias', 'segmentation_model.model.transformer.encoder.layer.0.attn.value.weight', 'segmentation_model.model.transformer.encoder.layer.0.attn.value.bias', 'segmentation_model.model.transformer.encoder.layer.0.attn.out.weight', 'segmentation_model.model.transformer.encoder.layer.0.attn.out.bias', 'segmentation_model.model.transformer.encoder.layer.1.attention_norm.weight', 'segmentation_model.model.transformer.encoder.layer.1.attention_norm.bias', 'segmentation_model.model.transformer.encoder.layer.1.ffn_norm.weight', 'segmentation_model.model.transformer.encoder.layer.1.ffn_norm.bias', 'segmentation_model.model.transformer.encoder.layer.1.ffn.fc1.weight', 'segmentation_model.model.transformer.encoder.layer.1.ffn.fc1.bias', 'segmentation_model.model.transformer.encoder.layer.1.ffn.fc2.weight', 'segmentation_model.model.transformer.encoder.layer.1.ffn.fc2.bias', 'segmentation_model.model.transformer.encoder.layer.1.attn.query.weight', 'segmentation_model.model.transformer.encoder.layer.1.attn.query.bias', 'segmentation_model.model.transformer.encoder.layer.1.attn.key.weight', 'segmentation_model.model.transformer.encoder.layer.1.attn.key.bias', 'segmentation_model.model.transformer.encoder.layer.1.attn.value.weight', 'segmentation_model.model.transformer.encoder.layer.1.attn.value.bias', 'segmentation_model.model.transformer.encoder.layer.1.attn.out.weight', 'segmentation_model.model.transformer.encoder.layer.1.attn.out.bias', 'segmentation_model.model.transformer.encoder.layer.2.attention_norm.weight', 'segmentation_model.model.transformer.encoder.layer.2.attention_norm.bias', 'segmentation_model.model.transformer.encoder.layer.2.ffn_norm.weight', 'segmentation_model.model.transformer.encoder.layer.2.ffn_norm.bias', 'segmentation_model.model.transformer.encoder.layer.2.ffn.fc1.weight', 'segmentation_model.model.transformer.encoder.layer.2.ffn.fc1.bias', 'segmentation_model.model.transformer.encoder.layer.2.ffn.fc2.weight', 'segmentation_model.model.transformer.encoder.layer.2.ffn.fc2.bias', 'segmentation_model.model.transformer.encoder.layer.2.attn.query.weight', 'segmentation_model.model.transformer.encoder.layer.2.attn.query.bias', 'segmentation_model.model.transformer.encoder.layer.2.attn.key.weight', 'segmentation_model.model.transformer.encoder.layer.2.attn.key.bias', 'segmentation_model.model.transformer.encoder.layer.2.attn.value.weight', 'segmentation_model.model.transformer.encoder.layer.2.attn.value.bias', 'segmentation_model.model.transformer.encoder.layer.2.attn.out.weight', 'segmentation_model.model.transformer.encoder.layer.2.attn.out.bias', 'segmentation_model.model.transformer.encoder.layer.3.attention_norm.weight', 'segmentation_model.model.transformer.encoder.layer.3.attention_norm.bias', 'segmentation_model.model.transformer.encoder.layer.3.ffn_norm.weight', 'segmentation_model.model.transformer.encoder.layer.3.ffn_norm.bias', 'segmentation_model.model.transformer.encoder.layer.3.ffn.fc1.weight', 'segmentation_model.model.transformer.encoder.layer.3.ffn.fc1.bias', 'segmentation_model.model.transformer.encoder.layer.3.ffn.fc2.weight', 'segmentation_model.model.transformer.encoder.layer.3.ffn.fc2.bias', 'segmentation_model.model.transformer.encoder.layer.3.attn.query.weight', 'segmentation_model.model.transformer.encoder.layer.3.attn.query.bias', 'segmentation_model.model.transformer.encoder.layer.3.attn.key.weight', 'segmentation_model.model.transformer.encoder.layer.3.attn.key.bias', 'segmentation_model.model.transformer.encoder.layer.3.attn.value.weight', 'segmentation_model.model.transformer.encoder.layer.3.attn.value.bias', 'segmentation_model.model.transformer.encoder.layer.3.attn.out.weight', 'segmentation_model.model.transformer.encoder.layer.3.attn.out.bias', 'segmentation_model.model.transformer.encoder.layer.4.attention_norm.weight', 'segmentation_model.model.transformer.encoder.layer.4.attention_norm.bias', 'segmentation_model.model.transformer.encoder.layer.4.ffn_norm.weight', 'segmentation_model.model.transformer.encoder.layer.4.ffn_norm.bias', 'segmentation_model.model.transformer.encoder.layer.4.ffn.fc1.weight', 'segmentation_model.model.transformer.encoder.layer.4.ffn.fc1.bias', 'segmentation_model.model.transformer.encoder.layer.4.ffn.fc2.weight', 'segmentation_model.model.transformer.encoder.layer.4.ffn.fc2.bias', 'segmentation_model.model.transformer.encoder.layer.4.attn.query.weight', 'segmentation_model.model.transformer.encoder.layer.4.attn.query.bias', 'segmentation_model.model.transformer.encoder.layer.4.attn.key.weight', 'segmentation_model.model.transformer.encoder.layer.4.attn.key.bias', 'segmentation_model.model.transformer.encoder.layer.4.attn.value.weight', 'segmentation_model.model.transformer.encoder.layer.4.attn.value.bias', 'segmentation_model.model.transformer.encoder.layer.4.attn.out.weight', 'segmentation_model.model.transformer.encoder.layer.4.attn.out.bias', 'segmentation_model.model.transformer.encoder.layer.5.attention_norm.weight', 'segmentation_model.model.transformer.encoder.layer.5.attention_norm.bias', 'segmentation_model.model.transformer.encoder.layer.5.ffn_norm.weight', 'segmentation_model.model.transformer.encoder.layer.5.ffn_norm.bias', 'segmentation_model.model.transformer.encoder.layer.5.ffn.fc1.weight', 'segmentation_model.model.transformer.encoder.layer.5.ffn.fc1.bias', 'segmentation_model.model.transformer.encoder.layer.5.ffn.fc2.weight', 'segmentation_model.model.transformer.encoder.layer.5.ffn.fc2.bias', 'segmentation_model.model.transformer.encoder.layer.5.attn.query.weight', 'segmentation_model.model.transformer.encoder.layer.5.attn.query.bias', 'segmentation_model.model.transformer.encoder.layer.5.attn.key.weight', 'segmentation_model.model.transformer.encoder.layer.5.attn.key.bias', 'segmentation_model.model.transformer.encoder.layer.5.attn.value.weight', 'segmentation_model.model.transformer.encoder.layer.5.attn.value.bias', 'segmentation_model.model.transformer.encoder.layer.5.attn.out.weight', 'segmentation_model.model.transformer.encoder.layer.5.attn.out.bias', 'segmentation_model.model.transformer.encoder.layer.6.attention_norm.weight', 'segmentation_model.model.transformer.encoder.layer.6.attention_norm.bias', 'segmentation_model.model.transformer.encoder.layer.6.ffn_norm.weight', 'segmentation_model.model.transformer.encoder.layer.6.ffn_norm.bias', 'segmentation_model.model.transformer.encoder.layer.6.ffn.fc1.weight', 'segmentation_model.model.transformer.encoder.layer.6.ffn.fc1.bias', 'segmentation_model.model.transformer.encoder.layer.6.ffn.fc2.weight', 'segmentation_model.model.transformer.encoder.layer.6.ffn.fc2.bias', 'segmentation_model.model.transformer.encoder.layer.6.attn.query.weight', 'segmentation_model.model.transformer.encoder.layer.6.attn.query.bias', 'segmentation_model.model.transformer.encoder.layer.6.attn.key.weight', 'segmentation_model.model.transformer.encoder.layer.6.attn.key.bias', 'segmentation_model.model.transformer.encoder.layer.6.attn.value.weight', 'segmentation_model.model.transformer.encoder.layer.6.attn.value.bias', 'segmentation_model.model.transformer.encoder.layer.6.attn.out.weight', 'segmentation_model.model.transformer.encoder.layer.6.attn.out.bias', 'segmentation_model.model.transformer.encoder.layer.7.attention_norm.weight', 'segmentation_model.model.transformer.encoder.layer.7.attention_norm.bias', 'segmentation_model.model.transformer.encoder.layer.7.ffn_norm.weight', 'segmentation_model.model.transformer.encoder.layer.7.ffn_norm.bias', 'segmentation_model.model.transformer.encoder.layer.7.ffn.fc1.weight', 'segmentation_model.model.transformer.encoder.layer.7.ffn.fc1.bias', 'segmentation_model.model.transformer.encoder.layer.7.ffn.fc2.weight', 'segmentation_model.model.transformer.encoder.layer.7.ffn.fc2.bias', 'segmentation_model.model.transformer.encoder.layer.7.attn.query.weight', 'segmentation_model.model.transformer.encoder.layer.7.attn.query.bias', 'segmentation_model.model.transformer.encoder.layer.7.attn.key.weight', 'segmentation_model.model.transformer.encoder.layer.7.attn.key.bias', 'segmentation_model.model.transformer.encoder.layer.7.attn.value.weight', 'segmentation_model.model.transformer.encoder.layer.7.attn.value.bias', 'segmentation_model.model.transformer.encoder.layer.7.attn.out.weight', 'segmentation_model.model.transformer.encoder.layer.7.attn.out.bias', 'segmentation_model.model.transformer.encoder.layer.8.attention_norm.weight', 'segmentation_model.model.transformer.encoder.layer.8.attention_norm.bias', 'segmentation_model.model.transformer.encoder.layer.8.ffn_norm.weight', 'segmentation_model.model.transformer.encoder.layer.8.ffn_norm.bias', 'segmentation_model.model.transformer.encoder.layer.8.ffn.fc1.weight', 'segmentation_model.model.transformer.encoder.layer.8.ffn.fc1.bias', 'segmentation_model.model.transformer.encoder.layer.8.ffn.fc2.weight', 'segmentation_model.model.transformer.encoder.layer.8.ffn.fc2.bias', 'segmentation_model.model.transformer.encoder.layer.8.attn.query.weight', 'segmentation_model.model.transformer.encoder.layer.8.attn.query.bias', 'segmentation_model.model.transformer.encoder.layer.8.attn.key.weight', 'segmentation_model.model.transformer.encoder.layer.8.attn.key.bias', 'segmentation_model.model.transformer.encoder.layer.8.attn.value.weight', 'segmentation_model.model.transformer.encoder.layer.8.attn.value.bias', 'segmentation_model.model.transformer.encoder.layer.8.attn.out.weight', 'segmentation_model.model.transformer.encoder.layer.8.attn.out.bias', 'segmentation_model.model.transformer.encoder.layer.9.attention_norm.weight', 'segmentation_model.model.transformer.encoder.layer.9.attention_norm.bias', 'segmentation_model.model.transformer.encoder.layer.9.ffn_norm.weight', 'segmentation_model.model.transformer.encoder.layer.9.ffn_norm.bias', 'segmentation_model.model.transformer.encoder.layer.9.ffn.fc1.weight', 'segmentation_model.model.transformer.encoder.layer.9.ffn.fc1.bias', 'segmentation_model.model.transformer.encoder.layer.9.ffn.fc2.weight', 'segmentation_model.model.transformer.encoder.layer.9.ffn.fc2.bias', 'segmentation_model.model.transformer.encoder.layer.9.attn.query.weight', 'segmentation_model.model.transformer.encoder.layer.9.attn.query.bias', 'segmentation_model.model.transformer.encoder.layer.9.attn.key.weight', 'segmentation_model.model.transformer.encoder.layer.9.attn.key.bias', 'segmentation_model.model.transformer.encoder.layer.9.attn.value.weight', 'segmentation_model.model.transformer.encoder.layer.9.attn.value.bias', 'segmentation_model.model.transformer.encoder.layer.9.attn.out.weight', 'segmentation_model.model.transformer.encoder.layer.9.attn.out.bias', 'segmentation_model.model.transformer.encoder.layer.10.attention_norm.weight', 'segmentation_model.model.transformer.encoder.layer.10.attention_norm.bias', 'segmentation_model.model.transformer.encoder.layer.10.ffn_norm.weight', 'segmentation_model.model.transformer.encoder.layer.10.ffn_norm.bias', 'segmentation_model.model.transformer.encoder.layer.10.ffn.fc1.weight', 'segmentation_model.model.transformer.encoder.layer.10.ffn.fc1.bias', 'segmentation_model.model.transformer.encoder.layer.10.ffn.fc2.weight', 'segmentation_model.model.transformer.encoder.layer.10.ffn.fc2.bias', 'segmentation_model.model.transformer.encoder.layer.10.attn.query.weight', 'segmentation_model.model.transformer.encoder.layer.10.attn.query.bias', 'segmentation_model.model.transformer.encoder.layer.10.attn.key.weight', 'segmentation_model.model.transformer.encoder.layer.10.attn.key.bias', 'segmentation_model.model.transformer.encoder.layer.10.attn.value.weight', 'segmentation_model.model.transformer.encoder.layer.10.attn.value.bias', 'segmentation_model.model.transformer.encoder.layer.10.attn.out.weight', 'segmentation_model.model.transformer.encoder.layer.10.attn.out.bias', 'segmentation_model.model.transformer.encoder.layer.11.attention_norm.weight', 'segmentation_model.model.transformer.encoder.layer.11.attention_norm.bias', 'segmentation_model.model.transformer.encoder.layer.11.ffn_norm.weight', 'segmentation_model.model.transformer.encoder.layer.11.ffn_norm.bias', 'segmentation_model.model.transformer.encoder.layer.11.ffn.fc1.weight', 'segmentation_model.model.transformer.encoder.layer.11.ffn.fc1.bias', 'segmentation_model.model.transformer.encoder.layer.11.ffn.fc2.weight', 'segmentation_model.model.transformer.encoder.layer.11.ffn.fc2.bias', 'segmentation_model.model.transformer.encoder.layer.11.attn.query.weight', 'segmentation_model.model.transformer.encoder.layer.11.attn.query.bias', 'segmentation_model.model.transformer.encoder.layer.11.attn.key.weight', 'segmentation_model.model.transformer.encoder.layer.11.attn.key.bias', 'segmentation_model.model.transformer.encoder.layer.11.attn.value.weight', 'segmentation_model.model.transformer.encoder.layer.11.attn.value.bias', 'segmentation_model.model.transformer.encoder.layer.11.attn.out.weight', 'segmentation_model.model.transformer.encoder.layer.11.attn.out.bias', 'segmentation_model.model.transformer.encoder.encoder_norm.weight', 'segmentation_model.model.transformer.encoder.encoder_norm.bias', 'segmentation_model.model.decoder.conv_more.0.weight', 'segmentation_model.model.decoder.conv_more.1.weight', 'segmentation_model.model.decoder.conv_more.1.bias', 'segmentation_model.model.decoder.conv_more.1.running_mean', 'segmentation_model.model.decoder.conv_more.1.running_var', 'segmentation_model.model.decoder.blocks.0.conv1.0.weight', 'segmentation_model.model.decoder.blocks.0.conv1.1.weight', 'segmentation_model.model.decoder.blocks.0.conv1.1.bias', 'segmentation_model.model.decoder.blocks.0.conv1.1.running_mean', 'segmentation_model.model.decoder.blocks.0.conv1.1.running_var', 'segmentation_model.model.decoder.blocks.0.conv2.0.weight', 'segmentation_model.model.decoder.blocks.0.conv2.1.weight', 'segmentation_model.model.decoder.blocks.0.conv2.1.bias', 'segmentation_model.model.decoder.blocks.0.conv2.1.running_mean', 'segmentation_model.model.decoder.blocks.0.conv2.1.running_var', 'segmentation_model.model.decoder.blocks.1.conv1.0.weight', 'segmentation_model.model.decoder.blocks.1.conv1.1.weight', 'segmentation_model.model.decoder.blocks.1.conv1.1.bias', 'segmentation_model.model.decoder.blocks.1.conv1.1.running_mean', 'segmentation_model.model.decoder.blocks.1.conv1.1.running_var', 'segmentation_model.model.decoder.blocks.1.conv2.0.weight', 'segmentation_model.model.decoder.blocks.1.conv2.1.weight', 'segmentation_model.model.decoder.blocks.1.conv2.1.bias', 'segmentation_model.model.decoder.blocks.1.conv2.1.running_mean', 'segmentation_model.model.decoder.blocks.1.conv2.1.running_var', 'segmentation_model.model.decoder.blocks.2.conv1.0.weight', 'segmentation_model.model.decoder.blocks.2.conv1.1.weight', 'segmentation_model.model.decoder.blocks.2.conv1.1.bias', 'segmentation_model.model.decoder.blocks.2.conv1.1.running_mean', 'segmentation_model.model.decoder.blocks.2.conv1.1.running_var', 'segmentation_model.model.decoder.blocks.2.conv2.0.weight', 'segmentation_model.model.decoder.blocks.2.conv2.1.weight', 'segmentation_model.model.decoder.blocks.2.conv2.1.bias', 'segmentation_model.model.decoder.blocks.2.conv2.1.running_mean', 'segmentation_model.model.decoder.blocks.2.conv2.1.running_var', 'segmentation_model.model.decoder.blocks.3.conv1.0.weight', 'segmentation_model.model.decoder.blocks.3.conv1.1.weight', 'segmentation_model.model.decoder.blocks.3.conv1.1.bias', 'segmentation_model.model.decoder.blocks.3.conv1.1.running_mean', 'segmentation_model.model.decoder.blocks.3.conv1.1.running_var', 'segmentation_model.model.decoder.blocks.3.conv2.0.weight', 'segmentation_model.model.decoder.blocks.3.conv2.1.weight', 'segmentation_model.model.decoder.blocks.3.conv2.1.bias', 'segmentation_model.model.decoder.blocks.3.conv2.1.running_mean', 'segmentation_model.model.decoder.blocks.3.conv2.1.running_var', 'segmentation_model.model.segmentation_head.0.weight', 'segmentation_model.model.segmentation_head.0.bias', 'blocks.0.cross_attn.in_proj_weight', 'blocks.0.cross_attn.in_proj_bias', 'blocks.0.cross_attn.out_proj.weight', 'blocks.0.cross_attn.out_proj.bias', 'blocks.1.cross_attn.in_proj_weight', 'blocks.1.cross_attn.in_proj_bias', 'blocks.1.cross_attn.out_proj.weight', 'blocks.1.cross_attn.out_proj.bias', 'blocks.2.cross_attn.in_proj_weight', 'blocks.2.cross_attn.in_proj_bias', 'blocks.2.cross_attn.out_proj.weight', 'blocks.2.cross_attn.out_proj.bias', 'blocks.3.cross_attn.in_proj_weight', 'blocks.3.cross_attn.in_proj_bias', 'blocks.3.cross_attn.out_proj.weight', 'blocks.3.cross_attn.out_proj.bias', 'blocks.4.cross_attn.in_proj_weight', 'blocks.4.cross_attn.in_proj_bias', 'blocks.4.cross_attn.out_proj.weight', 'blocks.4.cross_attn.out_proj.bias', 'blocks.5.cross_attn.in_proj_weight', 'blocks.5.cross_attn.in_proj_bias', 'blocks.5.cross_attn.out_proj.weight', 'blocks.5.cross_attn.out_proj.bias', 'blocks.6.cross_attn.in_proj_weight', 'blocks.6.cross_attn.in_proj_bias', 'blocks.6.cross_attn.out_proj.weight', 'blocks.6.cross_attn.out_proj.bias', 'blocks.7.cross_attn.in_proj_weight', 'blocks.7.cross_attn.in_proj_bias', 'blocks.7.cross_attn.out_proj.weight', 'blocks.7.cross_attn.out_proj.bias', 'blocks.8.cross_attn.in_proj_weight', 'blocks.8.cross_attn.in_proj_bias', 'blocks.8.cross_attn.out_proj.weight', 'blocks.8.cross_attn.out_proj.bias', 'blocks.9.cross_attn.in_proj_weight', 'blocks.9.cross_attn.in_proj_bias', 'blocks.9.cross_attn.out_proj.weight', 'blocks.9.cross_attn.out_proj.bias', 'blocks.10.cross_attn.in_proj_weight', 'blocks.10.cross_attn.in_proj_bias', 'blocks.10.cross_attn.out_proj.weight', 'blocks.10.cross_attn.out_proj.bias', 'blocks.11.cross_attn.in_proj_weight', 'blocks.11.cross_attn.in_proj_bias', 'blocks.11.cross_attn.out_proj.weight', 'blocks.11.cross_attn.out_proj.bias', 'head.weight', 'head.bias'], unexpected_keys=[])\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbrayan2221707\u001b[0m (\u001b[33mai-uis\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project 'colcaci' when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/BreastCATT/wandb/run-20250508_150709-1fk5g0yq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlegendary-sweep-2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ai-uis/colcaci\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ai-uis/colcaci/sweeps/8ali3gs5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ai-uis/colcaci/runs/1fk5g0yq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dataset_name' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_eval_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'output_dir' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'checkpointing_steps' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'use_cross_attn' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'use_segmentation' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'vit_version' was locked by 'sweep' (ignored update).\n",
      "Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 10.3MB/s]\n",
      "Downloading builder script: 100%|██████████| 7.56k/7.56k [00:00<00:00, 10.9MB/s]\n",
      "Downloading builder script: 100%|██████████| 7.38k/7.38k [00:00<00:00, 13.6MB/s]\n",
      " 67%|██████████████████████           | 24121/36120 [2:50:08<1:17:38,  2.58it/s]"
     ]
    }
   ],
   "source": [
    "! wandb agent ai-uis/colcaci/8ali3gs5 --count 10"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
